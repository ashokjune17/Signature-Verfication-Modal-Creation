{"cells":[{"metadata":{},"cell_type":"markdown","source":"Task:\n\nPFA\n\nA personâ€™s signature is representative of his identity. For us at the Bank, a signed document by a customer is an instruction from him for carrying out an approved transaction for him.\n\nOn on-boarding a customer we capture an image of his signature in our systems, and on receiving a signed document (Cheques, DDs, and others) from him we match the signature on the document with the one recorded in the database before proceeding with the instruction.\n\nIn the case of skilled forgeries, it becomes very difficult to verify the identity of the customer.\n\nWe want you to build a system that can help us distinguish forgeries from actual signatures. This system should be able to study signature parameters as strokes, curves, dots, dashes, writing fluidity & style, in a Writer-Independent manner and create features for identification of the signature.\n\nThe system should not use any existing APIs and should be completely self-developed.\n\nHow should it work?\n\nThe system shall work in 2 steps:\n\nStep 1: Accept & Store Genuine Signature Image: Take actual signature scanned image of the onboarding customer and store it in a database against a unique Customer ID\n\nStep 2: Accept & Compare Signature Images: Accept inputs of Customer ID and corresponding signature image. Compare with the signature stored in DB against the given Customer ID, and return a Confidence Match Score between the two signature images."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import all the necessary Library \nimport torchvision\nimport torch.utils.data as utils\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader,Dataset\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport torchvision.utils\nimport numpy as np\nimport time\nimport copy\nfrom torch.optim import lr_scheduler\nimport os\nfrom PIL import Image\nimport torch\nfrom torch.autograd import Variable\nimport PIL.ImageOps    \nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport pandas as pd \nfrom pathlib import Path\n\nuse_cuda = torch.cuda.is_available()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif not use_cuda:\n    print('No GPU found. Please use a GPU to train your neural network.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SignaturesNetwork(nn.Module):\n    def __init__(self):\n        super(SignaturesNetwork, self).__init__()\n        # Setting up the Sequential of CNN Layers\n        self.cnn1 = nn.Sequential(\n        nn.Conv2d(1, 96, kernel_size=11,stride=1),\n        nn.ReLU(inplace=True),\n        nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n        nn.MaxPool2d(3, stride=2),\n\n        nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2),\n        nn.ReLU(inplace=True),\n        nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n        nn.MaxPool2d(3, stride=2),\n        nn.Dropout2d(p=0.3),\n\n        nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(384,256 , kernel_size=3,stride=1,padding=1),\n        nn.ReLU(inplace=True),\n        nn.MaxPool2d(3, stride=2),\n        nn.Dropout2d(p=0.3),\n        )\n\n        # Defining the fully connected layers\n        self.fc1 = nn.Sequential(\n        # First Dense Layer\n        nn.Linear(30976, 1024),\n        nn.ReLU(inplace=True),\n        nn.Dropout2d(p=0.5),\n        # Second Dense Layer\n        nn.Linear(1024, 128),\n        nn.ReLU(inplace=True),\n        # Final Dense Layer\n        nn.Linear(128,2))\n\n    def forward_once(self, x):\n        # Forward pass \n        output = self.cnn1(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc1(output)\n        return output\n\n    def forward(self, input1, input2):\n        # forward pass of input 1\n        output1 = self.forward_once(input1)\n        # forward pass of input 2\n        output2 = self.forward_once(input2)\n        # returning the feature vectors of two inputs\n        return output1, output2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nhttps://hackernoon.com/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7\n\"\"\"\nclass ContrastiveLoss(torch.nn.Module):\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n        self.eps = 1e-9\n\n    def forward(self, output1, output2, label):\n        # Find the pairwise distance or eucledian distance of two output feature vectors\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        # perform contrastive loss calculation with the distance\n        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) + \n        (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n        \n        return loss_contrastive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import ImageEnhance\n\nclass SignaturesDataset():\n    \n    def __init__(self, train, genuine_path, training_dir=None, transform=None, enhance_factor=4.5):\n        # used to prepare the labels and images path\n        self.training_df = train\n        self.columns = [\"file\",\"signed_file\",\"status\"]\n        self.training_dir = training_dir    \n        self.transform = transform\n        self.genuine_path = genuine_path\n        self.enhance_factor = enhance_factor\n\n    def _enhance(self, image):\n        enhancer = ImageEnhance.Sharpness(image)\n        return enhancer.enhance(self.enhance_factor)\n\n    def __getitem__(self,index):\n        \n        # getting the image path\n        original_path=os.path.join(\n            self.genuine_path,\n            self.training_df.iat[index, 0])\n\n        compare_path=os.path.join(\n            self.training_dir,\n            self.training_df.iat[index, 1])\n        \n        target = torch.from_numpy(\n            np.array([int(self.training_df.iat[index, 2])],dtype=np.float32))\n\n        # Loading the image\n        original_img = self._enhance(Image.open(original_path))\n        compare_img = self._enhance(Image.open(compare_path))\n\n        original_img = original_img.convert(\"L\")\n        compare_img = compare_img.convert(\"L\")\n\n        #original_img = np.array(original_img)\n        #compare_img = np.array(compare_img)\n\n        data_transform = transforms.Compose([#transforms.ToPILImage(),\n                                            transforms.Resize((105,105), Image.ANTIALIAS),\n                                            #transforms.Grayscale(1),\n                                            #transforms.RandomResizedCrop(224),\n                                            #transforms.RandomHorizontalFlip(),\n                                            #transforms.RandomVerticalFlip(),\n                                            #transforms.RandomRotation(degrees=40), \n                                            transforms.ToTensor(),\n                                            #transforms.Normalize(mean=[0.456], std=[0.224])\n                                            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                            ])\n        \n        test_transform = transforms.Compose([#transforms.ToPILImage(),\n                                            transforms.Resize((105,105), Image.ANTIALIAS),\n                                            #transforms.Grayscale(1),\n                                            #transforms.Resize(256),\n                                            #transforms.CenterCrop(224),\n                                            transforms.ToTensor(),\n                                            #transforms.Normalize(mean=[0.456], std=[0.224])\n                                            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                                            ])\n        \n        # Apply image transformations\n        if self.transform == \"train\":\n            original_img = data_transform(original_img)\n            compare_img = data_transform(compare_img)\n        else:\n            original_img = test_transform(original_img)\n            compare_img = test_transform(compare_img)\n        \n        return original_img, compare_img , target\n    \n    def __len__(self):\n        return len(self.training_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(img,text=None,should_save=False):\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):\n    plt.plot(iteration,loss)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genuine_path = Path('/kaggle/input/signature-verification-dataset/sign_data/sign_data/train')\nforged_path = Path('/kaggle/input/signature-verification-dataset/sign_data/sign_data/test')\npath = Path('/kaggle/input/signature-verification-dataset/sign_data/sign_data/')\n\ntrain = pd.read_csv(path/\"train_data.csv\", low_memory=False)\ntest = pd.read_csv(path/\"test_data.csv\", low_memory=False)\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the the dataset from raw image folders\n\n#an = SignaturesDataset(train, genuine_path)\n#attrs = vars(an)\n#print(attrs)\n\nsignatures_dataset = SignaturesDataset(train, genuine_path, training_dir=genuine_path, transform=\"train\")\nsignatures_dataset_test = SignaturesDataset(test, forged_path, training_dir=forged_path, transform=\"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the sample of images and to check whether its loading properly\nvis_dataloader = DataLoader(signatures_dataset,\n                        shuffle=True,\n                        batch_size=8)\n\ndataiter = iter(vis_dataloader)\n\n\nexample_batch = next(dataiter)\nconcatenated = torch.cat((example_batch[0], example_batch[1]), 0)\nimshow(torchvision.utils.make_grid(concatenated))\nprint(example_batch[2].numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 40\nlearning_rate = 1e-4\nalpha = 0.99\nshow_every_n_batches = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the dataset as pytorch tensors using dataloader\ntrain_dataloader = DataLoader(signatures_dataset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=batch_size)\n\ntest_dataloader = DataLoader(signatures_dataset_test, num_workers=6, batch_size=1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n    if type(m) == nn.Conv2d:\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare Siamese Network\nnet = SignaturesNetwork()\n#net = SiameseNetwork()\n\nnet.apply(init_weights)\n\nif use_cuda:\n    net = net.to('cuda')\n    net = torch.nn.DataParallel(net)\n\n# Decalre Loss Function\nmargin = 2\ncriterion = ContrastiveLoss(margin)\n# Declare Optimizer\noptimizer = optim.RMSprop(net.parameters(), lr=learning_rate, alpha=alpha, eps=1e-8, weight_decay=0.0005, momentum=0.9)\n#optimizer = optim.Adam(net.parameters(),lr = learning_rate )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train():\n    counter = []\n    loss_history = []\n    iteration_number= 0\n    original_loss = np.inf\n    valid_loss = 0\n    total_loss = 0\n    \n    for epoch in range(epochs):\n        \n        for i, (img0, img1 , label) in enumerate(train_dataloader):\n            #img0, img1 , label = data\n            img0, img1 , label = img0.to('cuda', non_blocking=True), img1.to('cuda', non_blocking=True) , label.to('cuda', non_blocking=True)\n            \n            optimizer.zero_grad()\n            output1, output2 = net(img0, img1)\n            loss_contrastive = criterion(output1, output2, label)\n            loss_contrastive.backward()\n            optimizer.step()\n            \n            total_loss += loss_contrastive.item() * img0.size(0)\n            #valid_corrects += torch.sum(preds == target.data)\n            iteration_number += 1\n            \n        total_loss = total_loss/len(train_dataloader.dataset)\n        \n        if epoch%show_every_n_batches == 0:\n            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, total_loss, valid_loss))\n            counter.append(iteration_number)\n            loss_history.append(total_loss)\n            if total_loss < original_loss:\n                print('Training loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(original_loss,\n                total_loss))\n                with open('model.pt', 'wb') as pickle_file:\n                    torch.save(net.state_dict(), pickle_file)\n                original_loss = total_loss\n            total_loss = 0\n    show_plot(counter, loss_history)\n    return net","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = train()\ntorch.save(model.state_dict(), \"model.pt\")\nprint(\"Model Saved Successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[my Model](model.pt)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = net\nmodel.load_state_dict(torch.load(\"model.pt\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy=0\ncounter=0\ncorrect=0\n\nfor i, data in enumerate(test_dataloader,0):\n    \n    x0, x1, label = data    \n    # onehsot applies in the output of 128 dense vectors which is then converted to 2 dense vectors\n    \n    output1, output2 = model(x0.to(device),x1.to(device))\n    \n    res = nn.Softmax(dim=1)(output1.cuda() - output2.cuda())\n    label = label[0].tolist()\n    label = int(label[0])\n    result=torch.max(res,1)[1].data[0].tolist()\n    if label == result:\n        correct = correct+1\n    counter=counter+1\n    accuracy=(correct/len(test_dataloader))*100\n\nprint(\"Accuracy:{}%\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the sample outputs to view its dissimilarity\ncounter=0\nlist_1 = torch.FloatTensor([[1]])\nlist_0 = torch.FloatTensor([[0]])\n\nfor i, data in enumerate(test_dataloader,0):\n    x0, x1, label = data\n    concatenated = torch.cat((x0,x1),0)\n    output1, output2 = model(Variable(x0).to(device),Variable(x1).to(device))\n    \n    res = nn.Softmax(dim=1)(output1.cuda() - output2.cuda())\n    eucledian_distance_1 = F.pairwise_distance(output2, output1)\n    #eucledian_distance_2 = (output2 - output1).pow(2).sum(1)\n    result=torch.max(res,1)[1].data[0].tolist()\n    print(res)\n    #print(eucledian_distance_1.item())\n    #print(eucledian_distance_2)\n    \n    if result==1:\n        pred_label=\"Orginial\"\n    else:\n        pred_label=\"Forged\"\n        \n    if label==list_1:\n        label=\"Orginial\"\n    else:\n        label=\"Forged\"\n    \n    imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f} Label: {} predicted: {}'.format(eucledian_distance_1.item(),label, pred_label))\n    counter = counter + 1\n    if counter ==20:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}